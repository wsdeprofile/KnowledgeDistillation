## KD有哪些核心问题

1. KD的适用性：a. 输入的约束 b. T/S模型结构的约束 c. 任务场景的约束
2. KD算法的使用性： a. 知识来源 b. 如何从teacher transfer student，建立t/s的关联
3. KD的设计指导：哪些数据是有用的？
4. KD的结果保障：如何比其他技术做得好？
5. KD performance的证明，有没有统一的说法，还是各在各的数据集上+乱七八糟的各种架构ablsation study等等。





新工作：

在组里github账号下蒸馏KD的paper slides等等东西

看看序列数据和图数据，比如序列数据上non-autoregressive加蒸馏的效果可能会比auto-regressive好





### 《towards understanding KD》

三个指导性的原则：

data层面，回答什么样的data对KD是有效的。a) sample数量>>feature dimension  b) 训练student的样本点的分布要求样本点和teacher weight w*的夹角$\theta$的密度$p(\theta)$随着$\theta$增大而减小。

如何有效： student学习的权重$\hat w$可以完美模仿teacher权重w*；transfer risk存在理论上界0（也是完美模仿）。如果不满足n>>d的小样本情况，$\hat w$学到的是w\*在数据平面上的投影，tansfer risk与$p(\theta)$相关。

证明$\hat w$学到的最优解是w\* 在data span的投影：$\hat w$的梯度只在数据平面内（$\frac{\part w(\tau)}{\part \tau}=\frac{\part Loss(w)}{\part w}$连续梯度流）==> 目标函数可以无限优化下去，逼近最优解 ==> 最优解就是w\*在data span的投影。

  

a) 很容易满足，n>>d的场景下student 可以很好mimic teacher（但是缺少推导？？） b)可以指导如何从所有样本中抽出有效的样本训练student。

问题在于线性模型很好刻画w*，但是DNN很难。



优化算法层面，回答具体任务的优化目标与优化算法的设计。已被证明的是基于logits-KL散度的loss+梯度下降可以保证在数据平面内学习，并且可以持续优化直到loss降到最小positive value。优化目标最好具有单调性。

如果加上correct labels指导的优化目标，就有除了KD loss以外引入的optimization bias。n<d的情况，KD loss会有许多令优化目标=0的点，但它们泛化性能不同，bias越小越好。Q: 如果加其他任务势必引入另一个方向的梯度，如何考虑它的optimization bias。



### 《The State of KD in classification tasks》

idea: 许多KD方法无法复现performance，尤其是feature based。这里的baseline就是Hinton的蒸馏法。

RKD表现更好——因为基于hinton loss的额外的loss。

feature-based 缺乏良好的泛化性，可能只对特定的模型/参数环境/问题有用。

feature distillation techniques may not generalize and achieve underwhelming results when translated into a slightly different context. due to each method apart from KD for a particular model architecture, size, and training scheme.

Exploring the limits of knowledge distillation with a large amount of unsupervised data could be an interesting direction for future research。无监督蒸馏：在dataA上蒸馏，在dataB上测试，完全不是同一批数据。



### 9/10组会

Fitnet.  student隐层到teacher隐层映射，维度不同，空间不同，是否存在最后只学好了映射但没学好原本任务的情况？

直接映射中间层输出似乎不可控也没有道理，但是学习对输入的敏感度（attention transfer）可能更有泛化性。从结果来看activation layer比grad-based 好一点，可能的原因是如果用gradient的话直接学的就是输出对输入的敏感度，但是可能有些卷积核学得不好，有噪声，这些噪声依然会提供信息。但attention based的话就会“平均/聚合”一下，减小noise的影响。（把featuremap加一起就变成了attentionmap）

Factor transfer: student不会直接模拟teacher output，而是teacher的output首先通过autoencoder学出来一个编码向量，然后student output连一个encoder输出一个编码向量，让这个编码向量模拟teacher的那个。解决的问题：

数据组接受不到业务：不知道业务是什么，如何提供更好的数据？

solution1 pretrain-finetune:  在数据上自监督学特征，就不需要下游任务监督了。【pretrain-finetune原理是什么】

solution2 能不能pretrain一个teacher但是不finetune它？pretrain就会搞到很好的特征，线上最好能直接用简单的模型推理。teacher模型是一个利用自监督训练的大模型，并且可以一直在接受新数据更新。因为不知道下游任务所以使用自监督。student模型可以是一样的模型，或者小模型，它知道下游任务，但是可以利用teacher抽出来的信息，如feature based info.进行约束。





### 9/29组会

我的问题： 损失函数没看清就过？



CV方向

Face Recognition:  idea解决class太多导致的soft target sparse的问题。

高层neuro: 只考虑部分神经元？选择使能量函数最高的一些神经元传递给学生，相当于把特征压缩之后再使学生对齐。

high resolution指导low resolution: 怎么选择合适的特征表示传递给学生。去掉相关性比较高的特征，尽量保留每个类别最纯净的特征（使每个类别簇尽量远）



NLP方向

Sequence-level KD   从词汇级别升高到序列级别的蒸馏模型。NAT非自回归模型。词汇级别+自回归可能会导致误差累积，所以NAT直接对齐老师网络的知识。

Understanding KD in Non-autoregressive MT   知识蒸馏如何提升非自回归翻译模型——减少数据集的复杂度，帮助模型更好Model output variants。不论是哪种蒸馏都能收到这样的效果？

Autoregressive NMT是比较需要蒸馏的一个场景，因为需要搜索整个句子空间非常大。需要缩小搜索空间。通过KD缩小搜索空间。

BERT蒸馏：CV和NLP在对齐层数选择，对齐层数数量选择上可能存在差异？



其他方向

Transform from RNN to DNN: 

Adversial Distillation for Efficient Recommendation with External Knowledge:

通过KD引入用户评论等外部知识

MNIST上SN没见过6但是通过TN把6的特征传过来让它知道是6，可能是多个类似的特征组合



向高层的抽象：

从数据层面入手考虑蒸馏的有效性。soft target算是数据层面吗？

SN的样本比较小时，可能就比较依赖TN传递过来的信息，SN会从没见过的场景迁移一些知识过来。应该把哪些知识保留下来传递给SN。通过TN给SN构造更好的数据环境。

